\section{Глава 2. Методы машинного обучения с учителем}

В данной главе будут рассмотрены методы машинного обучения, используемые в работе, а также дана краткая характеристика задач, для которых эти методы используются.

\subsection{2.1. Виды машинного обучения с учителем}
Как правило, выделяют несколько видов машинного обучения с учителем. В данной работе используются:
\begin{enumerate}
\item Классификация. При решении задачи классификации предполагается, что имеется множество объектов разделенных некоторым образом на классы и, при этом, количество классов известно заранее.
\item Регрессия. В задаче регрессии требуется по заданному набору признаков объекта спрогнозировать целевую переменную, которая, как правило, может принимать любое вещественное значение.
\end{enumerate}

\subsection{2.2. Линейная регрессия}

Линейная регрессия \cite{linear regression} -- это линейная модель, которая устанавливает связь между зависимой переменной $y \in \mathbb{R}$ и одной или несколькими независимыми переменными $x \in \mathbb{R}^d$. Формально данная модель определяется следующим образом:
$$f(x)=w_0 + \sum_{j=1}^{d}x^j w_j$$
$x = (x^1, x^2, ..., x^d)$ -- входной вектор признаков \\
$w = (w_0, w_1, ... ,w_d)$ -- параметры модели

В задаче машинного обучения с учителем мы как правило имеем набор тренировочных данных $(x_1, y_1) ... (x_l,  y_l)$ с помощью которых мы оцениваем неизвестные параметры модели $w$. Самый популярный метод оценки неизвестных параметров модели линейной регрессии -- это метод наименьших квадратов, суть которого состоит в том, чтобы подобрать неизвестные параметры модели $w$ так, чтобы минимизировать функционал ошибки, который представляет собой среднеквадратичную ошибку алгоритма

$$Q(f, x) = \frac{1}{l}\sum_{i=1}^{l} (f(x_i) - y_i)$$
Минимизация данного функционала происходит с помощью оптимизационного подхода, а именно -- метода градиентного спуска \cite{gradient descent}.

\subsection{2.3. Логистическая регрессия}
Логистическая регрессия \cite{logistic regression} -- это метод классификации, который позволяет прогнозировать апостериорные вероятности принадлежности объектов классам с помощью линейной разделяющей гиперплоскости. 

Для того, чтобы оценивать апостериорные вероятности вводят так называемое отношение шансов, которое представляет из себя в случае бинарной классификации отношение вероятности принадлежности объекта $x$ к классу 1 ($P(G=1|x)$) к вероятности принадлежности объекта $x$ к классу 0 ($P(G=0|x)$). Логарифм данного отношения пытаются приблизить с помощью линейной функции

\begin{equation}\label{log_reg_eq}
\log \frac{P(G=1|x)}{1 - P(G=1|x)} = w_0 + \sum_{j=1}^{d}x^j w_j
\end{equation}
Делая некоторые преобразования в формуле  (\ref{log_reg_eq}) можно получить апостериорную оценку вероятности

$$P(G=1|x) = \frac{1}{1 + e^{-z}}$$
где $z = w_0 + \sum_{j=1}^{d}x^j w_j$

Параметры модели $w = (w_0, w_1, ... ,w_d)$ подбираются с помощью метода максимального правдоподобия.

\subsection{2.4. Метод опорных векторов}
Метод опорных векторов (support vector machine - SVM) \cite{svm}-- один из методов машинного обучения с учителем, который применяется для решения задач классификации и регрессии. 

Его идея заключается в том, чтобы каждый объект данных представить как вектор в $d$-мерном пространстве и разделить 2 класса (случай бинарной классификации) разделяющей гиперплоскостью. Однако, поскольку
гиперплоскостей может быть множество, то для получения оптимальной гиперплоскости решается задача максимизации зазора между классами для более уверенной классификации. В итоге задача сводится к поиску такой гиперплоскости, чтобы расстояние от нее до ближайшей точки было максимальным. Стоит заметить, что данный метод также расширяется и на случай многоклассовую классификацию.

Хотя этот метод относится к семейству линейных алгоритмов классификации, однако также можно получить нелинейную классификацию, используя трюк ядра (kernel trick), который основывается на предположении о том, что существует пространство большей размерности, в котором выборка линейно разделима. 

\subsection{2.5. Метод К ближайших соседей}

Метод K ближайших соседей \cite{knn} -- это метрический алгоритм, который может быть применим как к задаче классификации, так и к задаче регрессии. Его суть заключается в том, что объекту присваивается класс, который преобладает среди его соседей (задача классификации). Либо объекту присваивается значение, которое представляет собой среднее значение целевого атрибута среди соседей (задача регрессии). 

\subsection{2.6. Случайный лес}
Случайный лес \cite{random forest} является одним из алгоритмов, который основывается на ансамбле решающих деревьев. Данный алгоритм базируется на бэггинге и методе случайных подпространств. В задача классификации он определяет целевой класс как наиболее распространенный предсказанный класс среди всех деревьев. Что касается регрессии, то целевая переменная определятся как среднее значение среди всех деревьев.  

Данный метод имеет много достоинств, в числе которых эффективная обработка данных с большим числом признаков и классов, нечувствительность к масштабу признаков, возможность распараллеливания и масштабируемости, а также возможсть выявление наиболее информативных признаков.

Однако, помимо всех заявленных ранее преимуществ алгоритм построения случайного леса имеет и ряд недостатков. Обучение очень глубоких деревьев требует, как правило, много вычислительных ресурсов,
особенно, если мы имеем дело с большой выборкой или большим числом признаков. Но если ограничить глубину решающих деревьев в случайном лесе, то деревья решений не смогут улавливать сложные
закономерности в данных. \textbf{TODO перефразировать следующее предложение} Вторая проблема со случайным лесом состоит в том, что процесс построения деревьев является ненаправленным: каждое следующее дерево в композиции никак не зависит от предыдущих. Из-за этого для решения
сложных задач необходимо огромное количество деревьев.

\subsection{2.7. Градиентный бустинг}

Градиентный бустинг \cite{gradient boosting}- это техника машинного обучения для задач классификации и регрессии, которая строит модель в форме ансамбля слабых предсказывающих моделей, обычно деревьев решений. Главное отличие от случайного леса заключается в том, что в бустинге базовые модели строятся последовательно и каждый следующий алгоритм строится таким образом, чтобы исправить ошибки уже построенной композиции.

Однако главной проблемой градиентного бустинга является переобучение. \textbf{TODO}



\clearpage






