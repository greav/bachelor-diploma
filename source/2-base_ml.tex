\section{Глава 2. Методы машинного обучения с учителем}

В данной главе будут рассмотрены методы машинного обучения, используемые в работе, а также дана краткая характеристика задач, для которых эти методы используются.

\subsection{2.1. Виды машинного обучения с учителем}
Как правило, выделяют несколько видов машинного обучения с учителем. В данной работе используются:
\begin{enumerate}
\item Классификация. При решении задачи классификации предполагается, что имеется множество объектов разделенных некоторым образом на классы и, при этом, количество классов известно заранее.
\item Регрессия. В задаче регрессии требуется по заданному набору признаков объекта спрогнозировать целевую переменную, которая, как правило, может принимать любое вещественное значение.
\end{enumerate}

\subsection{2.2. Линейная регрессия}

Линейная регрессия \cite{linear regression} -- это линейная модель, которая устанавливает связь между зависимой переменной $y \in \mathbb{R}$ и одной или несколькими независимыми переменными $x \in \mathbb{R}^d$. Формально данная модель определяется следующим образом:
$$f(x)=w_0 + \sum_{j=1}^{d}x^j w_j$$
$x = (x^1, x^2, ..., x^d)$ -- входной вектор признаков \\
$w = (w_0, w_1, ... ,w_d)$ -- параметры модели

В задаче машинного обучения с учителем мы как правило имеем набор тренировочных данных $(x_1, y_1) ... (x_l,  y_l)$ с помощью которых мы оцениваем неизвестные параметры модели $w$. Самый популярный метод оценки неизвестных параметров модели линейной регрессии -- это метод наименьших квадратов, суть которого состоит в том, чтобы подобрать неизвестные параметры модели $w$ так, чтобы минимизировать функционал ошибки, который представляет собой среднеквадратичную ошибку алгоритма

$$Q(f, x) = \frac{1}{l}\sum_{i=1}^{l} (f(x_i) - y_i)$$
Минимизация данного функционала происходит с помощью оптимизационного подхода, а именно -- метода градиентного спуска.

Для решения задачи оценки возраста пользователя используется библиотека scikit learn \cite{sklearn}, которая имеет в своем составе уже различные реализации метода линейной регрессии, такие как Ridge, Lasso и ElasticNet. Данные 3 метода являются модификацией исходной модели линейной регрессии, к которой добавляются соответственно L1, L2 и комбинация L1 и L2 регуляризации.

Для автоматизации подбора параметров регуляризации из пакета scikit learn используется метод GridSearchCV с кросс-валидацией на 5 фолдов. При этом метрикой качества служит средняя абсолютная ошибка. 

Полученные результаты продемонстрированы в Таблице \ref{table1}.

\setlength\extrarowheight{8pt}
\begin{table}[h!]
\begin{tabular}{|c|m{5em}|m{5em}|m{5em}|l|}
\hline
\textbf{Выборка}                                                             & Исходная & Scaled  & OHE    & OHE + Scaled \\ [3ex] \hline  
\textbf{MAE}                                                                 & 5.4882   & 5.4871  & 3.6179 & 3.6127       \\ [2ex] \hline
\textbf{Регуляризация}                                                       & Ridge    & Ridge   & Ridge  & Ridge        \\ [2ex] \hline
\textbf{\begin{tabular}[c]{@{}c@{}}Коэффициент\\ регуляризации\end{tabular}} & 6.251    & 323.746 & 0.001  & 0.001        \\ [3ex] \hline
\end{tabular}
\caption{Результаты линейной регрессии для оценки возраста}
\label{table1}
\end{table}





\subsection{2.3. Логистическая регрессия}
Логистическая регрессия \cite{ESLII} -- это метод классификации, который позволяет прогнозировать апостериорные вероятности принадлежности объектов классам с помощью линейной разделяющей гиперплоскости. 

Для того, чтобы оценивать апостериорные вероятности вводят так называемое отношение шансов, которое представляет из себя в случае бинарной классификации отношение вероятности принадлежности объекта $x$ к классу 1 ($P(G=1|x)$) к вероятности принадлежности объекта $x$ к классу 0 ($P(G=0|x)$). Логарифм данного отношения пытаются приблизить с помощью линейной функции

\begin{equation}\label{log_reg_eq}
\log \frac{P(G=1|x)}{1 - P(G=1|x)} = w_0 + \sum_{j=1}^{d}x^j w_j
\end{equation}
Делая некоторые преобразования в формуле  (\ref{log_reg_eq}) можно получить апостериорную оценку вероятности

$$P(G=1|x) = \frac{1}{1 + e^{-z}}$$
где $z = w_0 + \sum_{j=1}^{d}x^j w_j$

Параметры модели $w = (w_0, w_1, ... ,w_d)$ подбираются с помощью метода максимального правдоподобия.

В данной работе логистическая регрессия используется для оценки пола пользователя социальной сети. Данный метод, также как и линеная регрессия находится в модуле linear\_models пакета scikit learn.  Результаты, полученные этим методом можно увидеть в Таблице \ref{table2}.

\setlength\extrarowheight{8pt}
\begin{table}[h!]
\centering
\begin{tabular}{|c|l|l|l|l|}
\hline
\textbf{Выборка}                                                           & Исходная & Scaled & OHE   & OHE + Scaled \\ \hline
\textbf{Accuracy}                                                          & 0.69     & 0.685  & 0.71  & 0.7          \\ \hline
\textbf{Precision}                                                         & 0.69     & 0.69   & 0.71  & 0.703        \\ \hline
\textbf{Recall}                                                            & 0.99     & 0.98   & 0.99  & 0.97         \\ \hline
\textbf{F1}                                                                & 0.81     & 0.81   & 0.823 & 0.82         \\ \hline
\textbf{ROC AUC}                                                           & 0.51     & 0.51   & 0.55  & 0.54         \\ \hline
\textbf{Регуляризация}                                                     & L1       & L1     & L1    & L1           \\ \hline
\textbf{\begin{tabular}[c]{@{}c@{}}Параметр \\ регуляризации\end{tabular}} & 0.0336   & 0.234  & 0.234 & 3792.69      \\ \hline
\end{tabular}
\caption{Результаты логистической регрессии}
\label{table2}
\end{table}

\subsection{2.4. Метод опорных векторов}
Метод опорных векторов (support vector machine - SVM) -- один из методов машинного обучения с учителем, который применяется для решения задач классификации \cite{svm} и регрессии \cite{svm regression}. 

Его идея заключается в том, чтобы каждый объект данных представить как вектор в $d$-мерном пространстве и разделить 2 класса (случай бинарной классификации) разделяющей гиперплоскостью. Однако, поскольку
гиперплоскостей может быть множество, то для получения оптимальной гиперплоскости решается задача максимизации зазора между классами для более уверенной классификации. В итоге задача сводится к поиску такой гиперплоскости, чтобы расстояние от нее до ближайшей точки было максимальным. Стоит заметить, что данный метод также расширяется и на случай многоклассовую классификацию.

Хотя этот метод относится к семейству линейных алгоритмов классификации, однако также можно получить нелинейную классификацию, используя трюк ядра (kernel trick), который основывается на предположении о том, что существует пространство большей размерности, в котором выборка линейно разделима. 

Для решения задачи оценки возраста и пола пользователя используется реализация метода SVM, которая находится в пакете scikit learn. Гиперпараметры моделей подбираются с помощью кросс-валидации. Результаты применения данного метода, продемонстрированы в Таблице \ref{table3} и Таблице \ref{table4}

\setlength\extrarowheight{8pt}
\begin{table}[h!]
\centering
\begin{tabular}{|l||l|l|l|l|}
\hline
\textbf{Выборка}                                                             & Исходная & Scaled   & OHE    & OHE + Scaled \\ \hline
\textbf{MAE}                                                                 & 4.84     & 3.526472 & 3.9893 & 5.521        \\ \hline
\textbf{\begin{tabular}[c]{@{}l@{}}Коэффициент\\ регуляризации\end{tabular}} & 2.0235   & 1000     & 10     & 10           \\ \hline
\end{tabular}
\caption{Результаты метода опорных векторов для оценки возраста}
\label{table3}
\end{table}


\begin{table}[h!]
\centering
\begin{tabular}{|c|l|l|l|l|}
\hline
\textbf{Выборка}                                                             & Исходная & Scaled & OHE    & OHE + Scaled \\ \hline
\textbf{Accuracy}                                                            & 0.685    & 0.7    & 0.685  & 0.71         \\ \hline
\textbf{Precision}                                                           & 0.685    & 0.6974 & 0.685  & 0.708        \\ \hline
\textbf{Recall}                                                              & 1.0      & 0.992  & 1.0    & 0.978        \\ \hline
\textbf{F1}                                                                  & 0.813    & 0.819  & 0.813  & 0.822        \\ \hline
\textbf{ROC AUC}                                                             & 0.5      & 0.528  & 0.5    & 0.552        \\ \hline
\textbf{\begin{tabular}[c]{@{}c@{}}Коэффициент\\ регуляризации\end{tabular}} & 0.0001   & 1.757  & 0.0001 & 0.5689       \\ \hline
\end{tabular}
\caption{Результаты метода опорных векторов для оценки пола}
\label{table4}
\end{table}



\subsection{2.5. Метод К ближайших соседей}

Метод K ближайших соседей (k-nearest neighbors - KNN) \cite{knn} -- это метрический алгоритм, который может быть применим как к задаче классификации, так и к задаче регрессии. Его суть заключается в том, что объекту присваивается класс, который преобладает среди его соседей (задача классификации). Либо объекту присваивается значение, которое представляет собой среднее значение целевого атрибута среди соседей (задача регрессии). 

Данный алгоритм бывает довольно эффективным если удачно подобрать гиперпараметры, такие как: число соседей, метрику близости и веса соседей. 

Если использовать взвешенный способ, то во внимание будет приниматься не только количество определённых классов попавших в область, но а также их удалённость от нового значения, что может повысить эффективность данного алгоритма.

Подбор параметров, как и ранее, был выполнен с помощью кросс-валидации и метода GridSearchCV библиотеки scikit learn.

Результаты представлены в Таблице \ref{table5} и в Таблице \ref{table6}.

\setlength\extrarowheight{8pt}
\begin{table}[h!]
\centering
\begin{tabular}{|c|l|l|}
\hline
\textbf{Выборка}      & Scaled     & OHE + Scaled \\ \hline
\textbf{MAE}          & 4.3338     & 4.63         \\ \hline
\textbf{weights}      & uniform    & uniform      \\ \hline
\textbf{n\_neighbors} & 9          & 8            \\ \hline
\textbf{p}            & 1          & 1            \\ \hline
\textbf{leaf\_size}   & 30         & 35           \\ \hline
\textbf{algorithm}    & ball\_tree & brute        \\ \hline
\end{tabular}
\caption{Результаты KNN для оценки возраста}
\label{table5}
\end{table}

\setlength\extrarowheight{8pt}
\begin{table}[h!]
\centering
\begin{tabular}{|c|l|l|}
\hline
\textbf{Выборка}     & Scaled    & OHE + Scaled \\ \hline
\textbf{Accuracy}    & 0.7       & 0.71         \\ \hline
\textbf{Precision}   & 0.7015    & 0.704        \\ \hline
\textbf{Recall}      & 0.978     & 0.992        \\ \hline
\textbf{F1}          & 0.817     & 0.824        \\ \hline
\textbf{ROC AUC}     & 0.536     & 0.543        \\ \hline
\textbf{weights}     & uniform   & uniform      \\ \hline
\textbf{n\_neighbors} & 49        & 41           \\ \hline
\textbf{p}           & 2         & 3            \\ \hline
\textbf{leaf\_size}   & 20        & 25           \\ \hline
\textbf{algorithm}   & ball\_tree & auto         \\ \hline
\end{tabular}
\caption{Результаты KNN для оценки пола}
\label{table6}
\end{table}



\subsection{2.6. Случайный лес}
Случайный лес \cite{random forest} является одним из алгоритмов, который основывается на ансамбле решающих деревьев. Данный алгоритм базируется на бэггинге и методе случайных подпространств. В задаче классификации он определяет целевой класс как наиболее распространенный предсказанный класс среди всех деревьев. Что касается регрессии, то целевая переменная определятся как среднее значение среди всех деревьев.  

Данный метод имеет много достоинств, в числе которых эффективная обработка данных с большим числом признаков и классов, нечувствительность к масштабу признаков, возможность распараллеливания и масштабируемости, а также возможсть выявление наиболее информативных признаков.

Однако, помимо всех заявленных ранее преимуществ алгоритм построения случайного леса имеет и ряд недостатков. Обучение очень глубоких деревьев требует, как правило, много вычислительных ресурсов,
особенно, если мы имеем дело с большой выборкой или большим числом признаков. Но если ограничить глубину решающих деревьев в случайном лесе, то деревья решений не смогут улавливать сложные
закономерности в данных. Другой проблемой можно считать то, что процесс построения деревьев является ненаправленным: каждое следующее дерево в композиции никак не зависит от предыдущих. Из-за этого для решения сложных задач необходимо огромное количество деревьев. 

Результаты представлены в Таблице \ref{table7} и в Таблице \ref{table8}.

\setlength\extrarowheight{8pt}
\begin{table}[h!]
\centering
\begin{tabular}{|c|l|}
\hline
\textbf{MAE}                 & 2.3151  \\ \hline
\textbf{n\_estimators}       & 800     \\ \hline
\textbf{min\_samples\_split} & 5        \\ \hline
\textbf{min\_samples\_leaf}  & 4        \\ \hline
\textbf{max\_features}       & sqrt     \\ \hline
\textbf{max\_depth}          & 90      \\ \hline
\textbf{bootstrap}           & True     \\ \hline
\end{tabular}
\caption{Результаты случайного леса для оценки возраста}
\label{table7}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{|c|l|}
\hline
\textbf{Выборка}             & Исходная \\ \hline
\textbf{Accuracy}            & 0.685    \\ \hline
\textbf{Precision}           & 0.6989   \\ \hline
\textbf{Recall}              & 0.9489   \\ \hline
\textbf{F1}                  & 0.8049   \\ \hline
\textbf{ROC AUC}             & 0.53     \\ \hline
\textbf{n\_estimators}       & 1600     \\ \hline
\textbf{min\_samples\_split} & 5        \\ \hline
\textbf{min\_samples\_leaf}  & 1        \\ \hline
\textbf{max\_features}       & sqrt     \\ \hline
\textbf{max\_depth}          & 10       \\ \hline
\end{tabular}
\caption{Результаты случайного леса для оценки пола}
\label{table8}
\end{table}

\subsection{2.7. Градиентный бустинг}

Градиентный бустинг \cite{gradient boosting} -- это техника машинного обучения для задач классификации и регрессии, которая строит модель в форме ансамбля слабых предсказывающих моделей, обычно деревьев решений. Главное отличие от случайного леса заключается в том, что в бустинге базовые модели строятся последовательно и каждый следующий алгоритм строится таким образом, чтобы исправить ошибки уже построенной композиции.

\textbf{http://neerc.ifmo.ru/wiki/index.php?title=XGBoost}

Данной метод имеет как преимущества, так и недостатки. Во-первых, главным плюсом является рассмотрение любого семейства базовых алгоритмов. А это дает широкие возможности для учета особенностей даннной задачи. Как правило, бустинг над решающими деревьями считается одним из эффективных вариантов использования данной модели.

Помимо этого, данный алгоритм является довольно простым и имеет четкое математическое обоснование, что позволяет в каждой конкретной вариации бустинга сделать некоторые математические и алгоритмические оптимизации, которые могут ускорить работу алгоритма.

Что касается недостатков, то,  как правило, бустинг работает достаточно медленно, поскольку требуется построение сотен или даже тысяч базовых методов, которые входят в композицию. Также, данный метод имеет свойство очень сильно подстраиваться под данные, в том числе под выбросы в данных, что ведет в свою очередь к переобучению. И, наконец, результаты работы бустинга довольно сложно интерпретировать, особенно
если в композицию входят десятки алгоритмов.

Для решения поставленных задача, а именно определение возраста и пола пользователя используется библиотека XGBoost \cite{xgboost}, которая является одной из самых популярных и эффективных реализаций градиентного бустинга на решающих деревьях. Подбор параметров выполнен с помощью кросс-валидации тренировочного множества. Полученные результаты можно наблюдать в Таблице \ref{table9} и Таблице \ref{table10}.

\setlength\extrarowheight{8pt}
\begin{table}[h!]
\centering
\begin{tabular}{|c|l|}
\hline
\textbf{Выборка}            & Исходная \\ \hline
\textbf{MAE}                & 2.29993  \\ \hline
\textbf{n\_estimators}      & 1473     \\ \hline
\textbf{learning\_rate}     & 0.01     \\ \hline
\textbf{max\_depth}         & 3        \\ \hline
\textbf{reg\_alpha}         & 138.94   \\ \hline
\textbf{reg\_lambda}        & 0.0167   \\ \hline
\textbf{min\_child\_weight} & 1        \\ \hline
\end{tabular}
\caption{Результаты градиентного бустинга для оценки возраста}
\label{table9}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{|c|l|}
\hline
\textbf{Выборка}            & Исходная \\ \hline
\textbf{Accuracy}           & 0.715    \\ \hline
\textbf{Precision}          & 0.724    \\ \hline
\textbf{Recall}             & 0.941    \\ \hline
\textbf{F1}                 & 0.819    \\ \hline
\textbf{ROC AUC}            & 0,581    \\ \hline
\textbf{n\_estimators}      & 4314     \\ \hline
\textbf{learning\_rate}     & 0.01     \\ \hline
\textbf{max\_depth}         & 3        \\ \hline
\textbf{reg\_alpha}         & 0.022    \\ \hline
\textbf{reg\_lambda}        & 1.0      \\ \hline
\textbf{min\_child\_weight} & 1        \\ \hline
\end{tabular}
\caption{Результаты градиентного бустинга для оценки пола}
\label{table10}
\end{table}



\clearpage






