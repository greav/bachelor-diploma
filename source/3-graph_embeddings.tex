\section{Глава 3. Графовые эмбеддинги}
В данной части будут рассмотрены алгоритмы векторизации вершин социального графа, а также их преимущества и недостатки.

\subsection{3.1. DeepWalk}

DeepWalk \cite{DeepWalk} -- это один из самых первых алгоритмов, который создает векторное представление узлов в графе с помощью случайных блужданий, а именно - идея заключается в переходе от структуры графа к некоторой последовательности вершин, которая сохранит основные свойства и информацию о графе. 

Данный метод является простым обобщением концепции word2vec \cite{word2vec} на графы, поскольку вершины в данном алгоритме можно рассматривать как слова, а случайные блуждания - как предложения. А поскольку было установлено, что степени вершин, появляющихся в случайных блужданиях подчиняются такому же закону распределения (степенному), что и распределение слов в естественном языке, то в итоге при обучении к сгенерированным случайным блужданиям можно применять языковые модели, а именно -- word2vec и SkipGram модели.  

Векторные представления вершин, найденные этим алгоритмом должны удовлетворять следующим 4 свойствам:

\begin{enumerate}
\item Адаптируемость. Т.е. векторные представления вершин социального графа не должны вынуждать нас пересчитывать их при добавлении новых социальных связей.
\item Социальная осведомленность. Расстояние между найденными скрытыми представлениями вершин социальной сети должно служить метрикой схожести пользователей.
\item Малое число измерений скрытых представлений. При мало количестве размещенных данных, модели с меньшим число измерений гораздо лучше обобщают данные и ускоряют сходимость алгоритма.
\item Непрерывность. \textbf{TODO}
\end{enumerate}

Решение, предложенное авторами оригинальной статьи удовлетворяет всем вышеприведенным характеристикам. Это решение состоит
из двух основных компонент: генерации случайной последовательности вершин (случайных блужданий) для каждой вершины $W_{v_i}$ и процедуры обновления скрытого представления.

Генерация случайного блуждания начинается с выбора начальной
вершины $v_1$ среди всех вершин $V$ графа $G$.  На $i$-ом шаге выбирается новая вершина $v_{i+1} \in N(v_i)$ среди соседей предыдущей вершины $v_i$. Данная процедура выполняется, пока длина последовательности
не достигнет заданного порога: $|W_{v_i}| = t$. При этом, данная процедура генерации случайных блужданий для каждой вершины выполняется $\gamma$ раз.

Поскольку целью языковой модели является оценка правдоподобия появления последовательности слов в корпусе, а случайные блуждания можно рассматривать как предложения в конкретном языке, то задача сводится к оценке правдоподобия вершины $v_i$ $P(v_i| v_0, v_1, ..., v_{i-1})$, если до этого мы встретили  $v_1, v_2, ..., v_{i-1}$ в случайном блуждании.

Так как на выходе мы должны получить векторное представление вершин графа, то вводится функция $\Phi: v \in V \mapsto R^{|V| \times d} $, которая сопоставляет каждой вершине $v$ графа $V$ его скрытое $d$-мерное представление. В итоге задача сводится к оценке вероятности:
$$P(v_i|\Phi(v_1), \Phi(v_2), ..., \Phi(v_{i-1}))$$



Последние исследования показали, что намного эффективнее, во-первых, предсказывать не слово по контексту, а контекст по слову.  Во-вторых, можно не учитывать порядок слов в контексте, что в итоге сводит нас к задаче минимизации следующего вида:

$$\min_{\Phi} -\log P(\{v_{i-w}, ..., v_{i_1}, v_{i+1}, ..., v_{i+w}\} | \Phi (v_i))$$
После генерации очередной последовательности вершин требуемой длины $t$ выполняется второй этап алгоритма -- обновление модели с помощью алгоритма SkipGram \cite{skipgram}. 

SkipGram - это языковая модель, которая максимизирует вероятность совместного появления слов, в заданном  окне $w$. Общая схема метода представлена в Алгоритме \ref{algo:1}:

\begin{algorithm}[H]
    \ForEach{$v_j \in W_{v_i}$}{
    	\ForEach{$u_k \in W_{v_i}[j - v: j + w]$}{
        	$J(\Phi) = -\log P(u_k | \Phi(v_j))$ \\
             $\Phi  = \Phi - \alpha * \frac{\partial J}{\partial \Phi}$
        
    	}
    }

    \caption{SkipGram($\Phi, W_{v_i}, w$)}
    \label{algo:1}
\end{algorithm}

Оценить вероятности $P(u_k | \Phi(v_j))$  можно используя либо логистический функционал, либо иерархический софтмакс. 

Оценка с помощью логистического функционала выглядит следующим образом:
$$P(u | \Phi(v)) = \frac{e^{\Phi(v)^T \cdot \Phi(u)}}{\sum_{v \in V} e^{\Phi(v)^T \cdot \Phi(u)}}$$

Однако проблема данного подхода заключается в том, что вычисление знаменателя требует значительных вычислительных ресурсов, поэтому для ускорения времени обучения используют иерархический софтмакс:

$$P(u_k | \Phi(v_j)) = \prod_{l=1}^{\lceil log|V| \rceil} P(b_l | \Phi(v_j))$$
Суть иерархического софтмакса заключается в том, что строится бинарное дерево, в листьях которого находятся вершины исходного графа. Поэтому вычисление требуемой вероятности эквивалентно вычислению вероятности пути в дереве от корня до вершины. 

В вышележащей формуле путь до вершины $u_k$ обозначается как ($b_0$, $b_1$, $...$, $b_{\lceil log|V| \rceil}$), где $b_0 = root$, $b_{\lceil log|V| \rceil} = u_k$. А вероятности $P(b_l | \Phi(v_j))$ моделируются с помощью бинарных классификаторов на вершине $b_{l-1}$, которые указывают в какую сторону и с какой вероятностью нужно идти по бинарному дереву. Таким образом вычислительная сложность оценки вероятности снижается с $O(|V |)$ до $O(log |V |)$.

\subsection{3.2. Node2Vec}




\clearpage