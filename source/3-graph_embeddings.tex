\section{Глава 3. Графовые эмбеддинги}
Графовые эмбеддинги -- это преобразование графов в вектор или набор векторов. Эмбеддинги должны охватывать топологию графа, отношение вершин к вершинам и другую соответствующую информацию о графах, подграфах и вершинах.

В данной части будут рассмотрены наиболее значимые алгоритмы векторизации вершин социального графа, а также их преимущества и недостатки.

Подходы для векторного представления узлов можно разделить на три основные группы: подходы факторизации, подходы случайного блуждания и глубокие подходы.

\subsection{3.1. DeepWalk}

DeepWalk \cite{DeepWalk} -- это один из самых первых алгоритмов, который создает векторное представление узлов в графе с помощью случайных блужданий, а именно - идея заключается в переходе от структуры графа к некоторой последовательности вершин, которая сохранит основные свойства и информацию о графе. 

Данный метод является простым обобщением концепции word2vec \cite{word2vec} на графы, поскольку вершины в данном алгоритме можно рассматривать как слова, а случайные блуждания - как предложения. А поскольку было установлено, что степени вершин, появляющихся в случайных блужданиях подчиняются такому же закону распределения (степенному), что и распределение слов в естественном языке, то в итоге при обучении к сгенерированным случайным блужданиям можно применять языковые модели, а именно -- word2vec и SkipGram модели.  


Решение, предложенное авторами оригинальной статьи состоит
из двух основных компонент: генерации случайной последовательности вершин (случайных блужданий) для каждой вершины $W_{v_i}$ и процедуры обновления скрытого представления.

Генерация случайного блуждания начинается с выбора начальной
вершины $v_1$ среди всех вершин $V$ графа $G$.  На $i$-ом шаге выбирается новая вершина $v_{i+1} \in N(v_i)$ среди соседей предыдущей вершины $v_i$. Данная процедура выполняется, пока длина последовательности
не достигнет заданного порога: $|W_{v_i}| = t$. При этом, данная процедура генерации случайных блужданий для каждой вершины выполняется $\gamma$ раз.

Поскольку целью языковой модели является оценка правдоподобия появления последовательности слов в корпусе, а случайные блуждания можно рассматривать как предложения в конкретном языке, то задача сводится к оценке правдоподобия вершины $v_i$ $P(v_i| v_0, v_1, ..., v_{i-1})$, если до этого мы встретили  $v_1, v_2, ..., v_{i-1}$ в случайном блуждании.

Так как на выходе мы должны получить векторное представление вершин графа, то вводится функция $\Phi: v \in V \mapsto R^{|V| \times d} $, которая сопоставляет каждой вершине $v$ графа $V$ его скрытое $d$-мерное представление. В итоге задача сводится к оценке вероятности:
$$P(v_i|\Phi(v_1), \Phi(v_2), ..., \Phi(v_{i-1}))$$



Последние исследования показали, что намного эффективнее, во-первых, предсказывать не слово по контексту, а контекст по слову.  Во-вторых, можно не учитывать порядок слов в контексте, что в итоге сводит нас к задаче минимизации следующего вида:

$$\min_{\Phi} -\log P(\{v_{i-w}, ..., v_{i_1}, v_{i+1}, ..., v_{i+w}\} | \Phi (v_i))$$
После генерации очередной последовательности вершин требуемой длины $t$ выполняется второй этап алгоритма -- обновление модели с помощью алгоритма SkipGram \cite{skipgram}. 

SkipGram - это языковая модель, которая максимизирует вероятность совместного появления слов, в заданном  окне $w$. Общая схема метода представлена в Алгоритме \ref{algo:1}:

\begin{algorithm}[H]
    \ForEach{$v_j \in W_{v_i}$}{
    	\ForEach{$u_k \in W_{v_i}[j - v: j + w]$}{
        	$J(\Phi) = -\log P(u_k | \Phi(v_j))$ \\
             $\Phi  = \Phi - \alpha * \frac{\partial J}{\partial \Phi}$
        
    	}
    }

    \caption{SkipGram($\Phi, W_{v_i}, w$)}
    \label{algo:1}
\end{algorithm}

Оценить вероятности $P(u_k | \Phi(v_j))$  можно используя либо логистический функционал, либо иерархический софтмакс. 

Оценка с помощью логистического функционала выглядит следующим образом:
$$P(u | \Phi(v)) = \frac{e^{\Phi(v)^T \cdot \Phi(u)}}{\sum_{v \in V} e^{\Phi(v)^T \cdot \Phi(u)}}$$

Что сводит задачу оптимизации к следующему виду:

$$
\max_{\Phi} \sum_{u \in V} -\log Z_u + \sum_{v \in N_w(u)} \Phi(v)^T \cdot \Phi(u)
$$

где 

\begin{equation} \label{Z_u}
Z_u = \sum_{v \in V} e^{ \Phi(v)^T \cdot \Phi(u)}
\end{equation}

Однако проблема данного подхода заключается в том, что вычисление знаменателя требует значительных вычислительных ресурсов, поэтому для ускорения времени обучения используют иерархический софтмакс:

$$P(u_k | \Phi(v_j)) = \prod_{l=1}^{\lceil log|V| \rceil} P(b_l | \Phi(v_j))$$
Суть иерархического софтмакса заключается в том, что строится бинарное дерево, в листьях которого находятся вершины исходного графа. Поэтому вычисление требуемой вероятности эквивалентно вычислению вероятности пути в дереве от корня до вершины. 

В вышележащей формуле путь до вершины $u_k$ обозначается как ($b_0$, $b_1$, $...$, $b_{\lceil log|V| \rceil}$), где $b_0 = root$, $b_{\lceil log|V| \rceil} = u_k$. А вероятности $P(b_l | \Phi(v_j))$ моделируются с помощью бинарных классификаторов на вершине $b_{l-1}$, которые указывают в какую сторону и с какой вероятностью нужно идти по бинарному дереву. Таким образом вычислительная сложность оценки вероятности снижается с $O(|V |)$ до $O(log |V |)$.

Однако, главным недостатком метода DeepWalk является то, что он требует больших вычислительных ресурсов, если поданный граф является большим (более 100 000 узлов) . С другой стороны, данный алгоритм можно распараллелить при генерации случайных блужданий. Другой недостаток -- обучение представлений происходит без взаимодействия с целевым значением исходной задачи. 

Результаты применения данного метода можно наблюдать в Таблице \ref{DeepWalk age table} и Таблице \ref{DeepWalk gender table}. 

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|l|}
\hline
\textbf{Выборка}                & Только эмбеддинги & Эмбеддинги + признаки \\ \hline
\textbf{Линейная регрессия}     &                   &                       \\ \hline
\textbf{Случайный лес}          &                   &                       \\ \hline
\textbf{Метод опорных векторов} &                   &                       \\ \hline
\textbf{К-ближайших соседей}    &                   &                       \\ \hline
\textbf{Градиентный бустинг}    &                   &                       \\ \hline
\end{tabular}
\caption{Результаты применения DeepWalk для определения возраста}
\label{DeepWalk age table}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|l|l|l|l|l|l|l|l|l|}
\hline
\textbf{Выборка}   & \multicolumn{5}{c|}{Только эмбеддинги} & \multicolumn{5}{l|}{Эмбеддинги + признаки} \\ \hline
\textbf{Метод}     & 1                      & 2 & 3 & 4 & 5 & 1      & 2      & 3      & 4      & 5      \\ \hline
\textbf{Accuracy}  &                        &   &   &   &   &        &        &        &        &        \\ \hline
\textbf{Precision} &                        &   &   &   &   &        &        &        &        &        \\ \hline
\textbf{Recall}    &                        &   &   &   &   &        &        &        &        &        \\ \hline
\textbf{F1}        &                        &   &   &   &   &        &        &        &        &        \\ \hline
\textbf{ROC AUC}   & \multicolumn{1}{l|}{}  &   &   &   &   &        &        &        &        &        \\ \hline
\end{tabular}
\caption{Результаты применения DeepWalk для определения пола}
\label{DeepWalk gender table}
\end{table}


\subsection{3.2. Node2Vec}

Поскольку метод DeepWalk выполняет случайные блуждания полностью случайным образом, то это означает, что эмбеддинги не сохраняют информации о локальной окрестности узла. Подход Node2vec призван исправить это.

Алгоритм Node2Vec \cite{Node2Vec} -- является модификацией DeepWalk с небольшой разницей в генерации случайных блужданиях и в обучении этих блужданий с помощью SkipGram модели. В данном подходе вводятся параметры $p$ и $q$, которые отвечают за баланс между обходом в глубину и в ширину графа. Параметр $q$ определяет, насколько вероятно, что случайное блуждание будет отдаляться от исходной вершины, а параметр $p$ отвечает, насколько вероятно вернуться к предыдущему узлу.

Вероятность перехода от вершины $u$ к вершине $v$ задается следующим образом: 

$$ P(v|u) =
\begin{cases}
\frac{\pi_{uv}}{Z}, & \text{если } (u, v) \in E \\
0, & \text{иначе}
\end{cases}
$$
где $\pi_{uv}$ -- это ненормализованная вероятность перехода из вершины $u$ в вершину $v$, а $Z$ -- нормировочная константа. Эта ненормализованная вероятность задается с помощью марковского процесса второго порядка, суть которого заключается в том, что вероятность перехода зависит от того, откуда мы пришли.

Обозначая за $w_{uv}$ вес ребра $(u, v)$, а за $d_{uv}$ -- количество ребер в кратчайшем пути из вершины $u$ в вершину $v$ , которое может принимать значения только из множества \{0, 1, 2\}. Тогда искомую вероятность $\pi_{uv}$ можно ввести как $\pi_{uv} = \alpha_{uv} \cdot w_{uv}$, где 

$$
\alpha_{uv} = 
\begin{cases}
\frac{1}{p}, & \text{если} \  d_{uv} = 0 \\
1, & \text{если}  \ d_{uv} = 1 \\
\frac{1}{q}, & \text{если} \ d_{uv} = 2
\end{cases}
$$

Если параметр $p > \max(1, q)$, то мы с меньшей вероятностью выберем уже посещенный узел. С другой стороны, если $p < min(1, q)$, то сгенерированные случайные блуждания будут близки к стартовой вершине. Аналогичны рассуждения для параметра $q$. Если $q > 1$, то случайное блуждание будет тяготиться к соседним вершинам и такие обходы будут близки к поиску в ширину. В противном случае, когда $q < 1$, случайные блуждания будут стремиться уйти от вершины $t$ \textbf{why t???}. Такое поведение отражает посик в глубину, который поощряет внешние исследования.

Другим отличие метода Node2Vec от DeepWalk является то, что вместо использования иерархического софтмакса для ускорения вычисления вероятности $P(u | \Phi(v))$ в SkipGram модели, он использует отрицательное сэмплирование. Идея заключается в том, чтобы уйти от подсчета суммы в формуле (\ref{Z_u}). Обозначая за $E$ множество, элементами которого являются пары слово-контекст, а за D -- сгенерированные случайным образом пары из распределения отрицательного сэмплирования, задача оптимизации в итоге сводится к максимизации следующего выражения:

$$
\sum_{(u, v) \in E} \log (\frac{1}{1 + e^{-\Phi(u)^T \Phi(v)}}) + \sum_{(u, v) \in D} \log (\frac{1}{1 + e^{\Phi(u)^T \Phi(v)}}) 
$$

Путем оптимизирования данного функционала, модель учится отличать зависимости между вершинами от шума в виде случайно
выбранных пар вершин.

Результаты применения данного метода можно наблюдать в Таблице \ref{Node2Vec age table} и Таблице \ref{Node2Vec gender table}. 

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|l|}
\hline
\textbf{Выборка}                & Только эмбеддинги & Эмбеддинги + признаки \\ \hline
\textbf{Линейная регрессия}     &                   &                       \\ \hline
\textbf{Случайный лес}          &                   &                       \\ \hline
\textbf{Метод опорных векторов} &                   &                       \\ \hline
\textbf{К-ближайших соседей}    &                   &                       \\ \hline
\textbf{Градиентный бустинг}    &                   &                       \\ \hline
\end{tabular}
\caption{Результаты применения Node2Vec для определения возраста}
\label{Node2Vec age table}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|l|l|l|l|l|l|l|l|l|}
\hline
\textbf{Выборка}   & \multicolumn{5}{c|}{Только эмбеддинги} & \multicolumn{5}{l|}{Эмбеддинги + признаки} \\ \hline
\textbf{Метод}     & 1                      & 2 & 3 & 4 & 5 & 1      & 2      & 3      & 4      & 5      \\ \hline
\textbf{Accuracy}  &                        &   &   &   &   &        &        &        &        &        \\ \hline
\textbf{Precision} &                        &   &   &   &   &        &        &        &        &        \\ \hline
\textbf{Recall}    &                        &   &   &   &   &        &        &        &        &        \\ \hline
\textbf{F1}        &                        &   &   &   &   &        &        &        &        &        \\ \hline
\textbf{ROC AUC}   & \multicolumn{1}{l|}{}  &   &   &   &   &        &        &        &        &        \\ \hline
\end{tabular}
\caption{Результаты применения Node2Vec для определения пола}
\label{Node2Vec gender table}
\end{table}




\clearpage