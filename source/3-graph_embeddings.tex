\section{Глава 3. Графовые эмбеддинги}
Под графовыми эмбеддингами понимается преобразование графов в вектор или набор векторов. Эмбеддинги должны охватывать топологию графа, отношение между вершинами и другую соответствующую информацию о графах, подграфах и вершинах.

В данной части будут рассмотрены наиболее значимые алгоритмы векторизации вершин социального графа, а также их преимущества и недостатки.

\subsection{3.1. DeepWalk}

DeepWalk \cite{DeepWalk} -- это один из самых первых алгоритмов, который создает векторное представление узлов в графе с помощью случайных блужданий, а именно -- идея заключается в переходе от структуры графа к некоторой последовательности вершин, которая сохранит основные свойства и информацию о графе. 

Данный метод является простым обобщением концепции word2vec \cite{word2vec} на графы, поскольку вершины в данном алгоритме можно рассматривать как слова, а случайные блуждания - как предложения. А поскольку было установлено, что степени вершин, появляющихся в случайных блужданиях подчиняются такому же закону распределения (степенному), что и распределение слов в естественном языке, то в итоге при обучении к сгенерированным случайным блужданиям можно применять языковые модели, а именно -- word2vec и SkipGram модели.  


Решение, предложенное авторами оригинальной статьи \cite{DeepWalk} состоит
из двух основных компонент: генерации случайной последовательности вершин (случайных блужданий) $W_{v_i}$ для каждой вершины $v_i$ и процедуры обновления скрытого представления.

Генерация случайного блуждания начинается с выбора начальной
вершины $v_1$ среди всех вершин $V$ графа $G$.  На $i$-ом шаге выбирается новая вершина $v_{i+1} \in N(v_i)$ среди соседей предыдущей вершины $v_i$. Данная процедура выполняется, пока длина последовательности
не достигнет заданного порога: $|W_{v_i}| = t$. При этом, данная процедура генерации случайных блужданий для каждой вершины выполняется $\gamma$ раз.

Поскольку целью языковой модели является оценка правдоподобия появления последовательности слов в корпусе, а случайные блуждания можно рассматривать как предложения в конкретном языке, то задача сводится к оценке правдоподобия вершины $v_i$ $P(v_i| v_0, v_1, ..., v_{i-1})$, если до этого мы встретили  $v_1, v_2, ..., v_{i-1}$ в случайном блуждании.

Так как на выходе мы должны получить векторное представление вершин графа, то вводится функция $\Phi: v \in V \mapsto R^{|V| \times d} $, которая сопоставляет каждой вершине $v$ графа $V$ его скрытое $d$-мерное представление. В итоге задача сводится к оценке вероятности:
$$P(v_i|\Phi(v_1), \Phi(v_2), ..., \Phi(v_{i-1}))$$



Исследования алгоритма word2vec показали, что намного эффективнее, во-первых, предсказывать не слово по контексту, а контекст по слову (в случае исходной задачи под словом понимается вершина графа, а под контекстом последовательность вершин в случайном блуждании).  Во-вторых, можно не учитывать порядок слов в контексте, что в итоге сводит нас к задаче минимизации следующего вида:

$$\min_{\Phi} -\log P(\{v_{i-w}, ..., v_{i_1}, v_{i+1}, ..., v_{i+w}\} | \Phi (v_i))$$

После генерации очередной последовательности вершин требуемой длины $t$ выполняется второй этап алгоритма -- обновление модели с помощью алгоритма SkipGram.

SkipGram - это языковая модель, которая максимизирует вероятность совместного появления слов, в заданном  окне $w$. Общая схема метода представлена в Алгоритме \ref{algo:1}:

\begin{algorithm}[H]
    \ForEach{$v_j \in W_{v_i}$}{
    	\ForEach{$u_k \in W_{v_i}[j - v: j + w]$}{
        	$J(\Phi) = -\log P(u_k | \Phi(v_j))$ \\
             $\Phi  = \Phi - \alpha * \frac{\partial J}{\partial \Phi}$
        
    	}
    }

    \caption{SkipGram($\Phi, W_{v_i}, w$)}
    \label{algo:1}
\end{algorithm}

Оценить вероятности $P(u_k | \Phi(v_j))$  можно используя либо логистический функционал, либо иерархический софтмакс. 

Оценка с помощью логистического функционала выглядит следующим образом:
$$P(u_k | \Phi(v_j)) = \frac{e^{\Phi(v_j)^T \cdot \Phi(u_k)}}{\sum_{v \in V} e^{\Phi(v)^T \cdot \Phi(u_k)}}$$

Что сводит задачу оптимизации к следующему виду:

$$
\max_{\Phi} \sum_{u \in V} -\log Z_u + \sum_{v \in N_w(u)} \Phi(v)^T \cdot \Phi(u)
$$

где 

\begin{equation} \label{Z_u}
Z_u = \sum_{v \in V} e^{ \Phi(v)^T \cdot \Phi(u)}
\end{equation}

Однако проблема данного подхода заключается в том, что вычисление знаменателя требует значительных вычислительных ресурсов, поэтому для ускорения времени обучения используют иерархический софтмакс:

$$P(u_k | \Phi(v_j)) = \prod_{l=1}^{\lceil log|V| \rceil} P(b_l | \Phi(v_j))$$
Суть иерархического софтмакса заключается в том, что строится бинарное дерево, в листьях которого находятся вершины исходного графа. Поэтому вычисление требуемой вероятности эквивалентно вычислению вероятности пути в дереве от корня до вершины. 

В вышележащей формуле путь до вершины $u_k$ обозначается как ($b_0$, $b_1$, $...$, $b_{\lceil log|V| \rceil}$), где $b_0 = root$, $b_{\lceil log|V| \rceil} = u_k$. А вероятности $P(b_l | \Phi(v_j))$ моделируются с помощью бинарных классификаторов на вершине $b_{l-1}$, которые указывают в какую сторону и с какой вероятностью нужно идти по бинарному дереву. Таким образом вычислительная сложность оценки вероятности снижается с $O(|V |)$ до $O(log |V |)$.

Однако, главным недостатком метода DeepWalk является то, что он требует больших вычислительных ресурсов, если поданный на вход граф является большим (более 100 000 узлов). С другой стороны, данный алгоритм можно распараллелить при генерации случайных блужданий, но это не дает значимых улучшений производительности. Другой недостаток -- обучение векторных представлений происходит без взаимодействия с целевым значением исходной задачи. 

Для решения задачи векторного представления вершин графа была использована одноименная библиотека deepwalk \cite{deepwalk lib}. При этом использование данного алгоритма проводилось со следующими параметрами:  $\gamma = 30$ (число случайных блужданий), $t = 40$ (длина случайных блужданий), $w = 10$ (размер окна в алгоритме SkipGram), $d = 64$ (размерность полученных векторов). Результаты применения данного метода для задач определения возраста и пола можно наблюдать в Таблице \ref{DeepWalk age table} и в Таблице \ref{DeepWalk gender table}. 

По полученным результатам можно заметить, что уже использование одних эмбеддингов в алгоритмах машинного обучения дает неплохие результаты, а при совместном использовании исходных признаков и эмбеддингов результаты становятся намного лучше. Так, например, использование градиентного бустинга в задаче определения возраста на исходных признаках и эмбеддингах дало внушительные  результаты по средней абсолютной ошибке (MAE < 2).  

В задаче определения возраста были также достигнуты результаты, превосходящие базовые алгоритмы на исходных данных.

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|l|}
\hline
\textbf{}                       & \multicolumn{2}{c|}{MAE}                  \\ \hline
\textbf{Выборка}                & Только эмбеддинги & Признаки + эмбеддинги \\ \hline
\textbf{Линейная регрессия}     & 5.538             & 3.75                  \\ \hline
\textbf{Случайный лес}          & 4.36              & 2.6                   \\ \hline
\textbf{Метод опорных векторов} & 3.9               & 4.28                  \\ \hline
\textbf{k-ближайших соседей}    & 4.87              & 4.42                  \\ \hline
\textbf{Градиентный бустинг}    & 4.68              & 1.37     \\ \hline
\end{tabular}
\caption{Результаты применения DeepWalk для определения возраста}
\label{DeepWalk age table}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|l|l|l|l|l|l|l|l|l|}
\hline
\textbf{Выборка}   & \multicolumn{5}{c|}{Только эмбеддинги} & \multicolumn{5}{l|}{Признаки + эмбеддинги} \\ \hline
\textbf{Метод}     & 1   & 2      & 3      & 4     & 5      & 1   & 2       & 3       & 4       & 5      \\ \hline
\textbf{Accuracy}  & 0.665    & 0.735  & 0.685  & 0.72  & 0.715  &  0.715   & 0.775   & 0.715   & 0.765   & 0.75   \\ \hline
\textbf{Precision} &   0.17  & 0.678  & 0.5    & 0.57  & 0.63   &   0.6  & 0.821   & 0.64    & 0.68    & 0.81   \\ \hline
\textbf{Recall}    &  0.016  & 0.301  & 0.1    & 0.43  & 0.22   &  0.29   & 0.36    & 0.22    & 0.48    & 0.27   \\ \hline
\textbf{F1}        & 0.03    & 0.417  & 0.16   & 0.49  & 0.33   &  0.39   & 0.51    & 0.33    & 0.56    & 0.4    \\ \hline
\end{tabular}
\caption{Результаты применения DeepWalk для определения пола (1 - логистическая регрессия, 2 - случайный лес, 3 - метод опорных векторов, 4 - k-ближайших соседей, 5 - градиентный бустинг)}
\label{DeepWalk gender table}
\end{table}



\subsection{3.2. Node2Vec}

Поскольку метод DeepWalk выполняет случайные блуждания полностью случайным образом, то это означает, что эмбеддинги не сохраняют информации о локальной окрестности узла. Подход Node2vec \cite{Node2Vec}  призван исправить это.

Алгоритм Node2Vec -- является модификацией DeepWalk с небольшой разницей в генерации случайных блужданиях и в обучении этих блужданий с помощью SkipGram модели. В данном подходе вводятся параметры $p$ и $q$, которые отвечают за баланс между обходом в глубину и в ширину графа. Параметр $q$ определяет, насколько вероятно, что случайное блуждание будет отдаляться от исходной вершины, а параметр $p$ отвечает насколько вероятно вернуться к предыдущему узлу.

Вероятность перехода от вершины $u$ к вершине $v$ задается следующим образом: 

$$ P(v|u) =
\begin{cases}
\frac{\pi_{uv}}{Z}, & \text{если } (u, v) \in E \\
0, & \text{иначе}
\end{cases}
$$
где $\pi_{uv}$ -- это ненормализованная вероятность перехода из вершины $u$ в вершину $v$, а $Z$ -- нормировочная константа. Эта ненормализованная вероятность задается с помощью марковского процесса второго порядка, суть которого заключается в том, что вероятность перехода зависит от того, откуда мы пришли.

Обозначая за $w_{uv}$ вес ребра $(u, v)$, а за $d_{uv}$ -- количество ребер в кратчайшем пути из вершины $u$ в вершину $v$ , которое может принимать значения только из множества \{0, 1, 2\}, то тогда искомую вероятность $\pi_{uv}$ можно ввести как $\pi_{uv} = \alpha_{uv} \cdot w_{uv}$, где 

$$
\alpha_{uv} = 
\begin{cases}
\frac{1}{p}, & \text{если} \  d_{uv} = 0 \\
1, & \text{если}  \ d_{uv} = 1 \\
\frac{1}{q}, & \text{если} \ d_{uv} = 2
\end{cases}
$$

Если параметр $p > \max(1, q)$, то мы с меньшей вероятностью выберем уже посещенный узел. С другой стороны, если $p < min(1, q)$, то сгенерированные случайные блуждания будут близки к стартовой вершине. Аналогичны рассуждения для параметра $q$. Если $q > 1$, то случайное блуждание будет тяготиться к соседним вершинам и такие обходы будут близки к поиску в ширину. В противном случае, когда $q < 1$, случайные блуждания будут стремиться уйти от текущей вершины как можно дальше. Такое поведение отражает посик в глубину, который поощряет внешние исследования.

Другим отличием метода Node2Vec от DeepWalk является то, что вместо использования иерархического софтмакса для ускорения вычисления вероятности $P(u | \Phi(v))$ в SkipGram модели, он использует отрицательное сэмплирование. Идея заключается в том, чтобы уйти от подсчета суммы в формуле (\ref{Z_u}). Обозначая за $E$ множество, элементами которого являются пары слово-контекст (узел-случайное блуждание), а за D -- сгенерированные случайным образом пары из распределения отрицательного сэмплирования, задача оптимизации в итоге сводится к максимизации следующего выражения:

$$
\sum_{(u, v) \in E} \log (\frac{1}{1 + e^{-\Phi(u)^T \Phi(v)}}) + \sum_{(u, v) \in D} \log (\frac{1}{1 + e^{\Phi(u)^T \Phi(v)}}) 
$$

Путем оптимизирования данного функционала, модель учится отличать зависимости между вершинами от шума в виде случайно
выбранных пар вершин.


\begin{table}[!htb]
\centering
\begin{tabular}{|c|c|l|}
\hline
\textbf{}                       & \multicolumn{2}{c|}{MAE}                  \\ \hline
\textbf{Выборка}                & Только эмбеддинги & Признаки + эмбеддинги \\ \hline
\textbf{Линейная регрессия}     &    5.77               &     4.23                  \\ \hline
\textbf{Случайный лес}          &    6.21               &    2.8                   \\ \hline
\textbf{Метод опорных векторов} &      5.56             &      5.59                 \\ \hline
\textbf{К-ближайших соседей}    &       4.7            &        4.7               \\ \hline
\textbf{Градиентный бустинг}    &      5.82             &        2.7               \\ \hline
\end{tabular}
\caption{Результаты применения Node2Vec для определения возраста}
\label{Node2Vec age table}
% \quad
% \begin{tabular}{|c|c|l|l|l|l|l|l|l|l|l|}
% \hline
% \textbf{Выборка}   & \multicolumn{5}{c|}{Только эмбеддинги} & \multicolumn{5}{l|}{Признаки + эмбеддинги} \\ \hline
% \textbf{Метод}     & 1                      & 2 & 3 & 4 & 5 & 1      & 2      & 3      & 4      & 5      \\ \hline
% \textbf{Accuracy}  &     0.64                   & 0.615  & 0.71   & 0.735  & 0.715  &   0.665     &  0.59      &0.71        & 0.765        &  0.73      \\ \hline
% \textbf{Precision} &       0.36                &  0.375 & 1.0  & 0.61  & 0.64  &     0.4   &   0.38     &   0.75     &  0.79      & 0.71       \\ \hline
% \textbf{Recall}    &      0.19                  & 0.33  & 0.08   & 0.43  & 0.22  &    0.13    &   0.48     &  0.1       &       0.35  &   0.24    \\ \hline
% \textbf{F1}        &     0.25                    &  0.35 & 0.15   &  0.5 &  0.33 &    0.19    &     0.42   &   0.17     &  0.48      & 0.36        \\ \hline
% \end{tabular}
% \caption{Результаты применения Node2Vec для определения пола (1 - логистическая регрессия, 2 - случайный лес, 3 - метод опорных векторов, 4 - k-ближайших соседей, 5 - градиентный бустинг)}
% \label{Node2Vec gender table}
\end{table}

Для нахождения векторных представлений вершин была выбрана одноименная библиотека node2vec \cite{node2vec lib}, которая реализована на языке программирования python.  В качестве параметров модели Node2Vec были взяты: $\gamma = 30$, $t = 40$, $w = 10$, $d = 64$, $p = 0.8$, $q = 0.5$. 
Результаты применения данного метода к задачам определения возраста и пола можно наблюдать в Таблице \ref{Node2Vec age table} и в Таблице \ref{Node2Vec gender table}. 

По полученным результатам можно сделать вывод, что в целом совместное использование признаков и эмбеддингов улучшает результаты в сравнении с использованием одних лишь эмбеддингов, однако результаты применения DeepWalk оказались в целом лучше, чем у Node2Vec.


\begin{table}[h!]
\centering
\begin{tabular}{|c|c|l|l|l|l|l|l|l|l|l|}
\hline
\textbf{Выборка}   & \multicolumn{5}{c|}{Только эмбеддинги} & \multicolumn{5}{l|}{Признаки + эмбеддинги} \\ \hline
\textbf{Метод}     & 1                      & 2 & 3 & 4 & 5 & 1      & 2      & 3      & 4      & 5      \\ \hline
\textbf{Accuracy}  &     0.64                   & 0.615  & 0.71   & 0.735  & 0.715  &   0.665     &  0.59      &0.71        & 0.765        &  0.73      \\ \hline
\textbf{Precision} &       0.36                &  0.375 & 1.0  & 0.61  & 0.64  &     0.4   &   0.38     &   0.75     &  0.79      & 0.71       \\ \hline
\textbf{Recall}    &      0.19                  & 0.33  & 0.08   & 0.43  & 0.22  &    0.13    &   0.48     &  0.1       &       0.35  &   0.24    \\ \hline
\textbf{F1}        &     0.25                    &  0.35 & 0.15   &  0.5 &  0.33 &    0.19    &     0.42   &   0.17     &  0.48      & 0.36        \\ \hline
\end{tabular}
\caption{Результаты применения Node2Vec для определения пола (1 - логистическая регрессия, 2 - случайный лес, 3 - метод опорных векторов, 4 - k-ближайших соседей, 5 - градиентный бустинг)}
\label{Node2Vec gender table}
\end{table}

\clearpage



