\section{Глава 4. Графовые нейронные сети}
Графовые нейронные сети (Graph Neural Network - GNN) -- это методы глубокого обучения, основанные на графах. Как правило, они базируются на следующих фундаментальных аспектах: сверточных нейронных сетях (Convolutional Neural Network - CNN) и графовых эмбеддингах. 

Сверточные нейронные сети могут работать только с евклидовыми данными, такими как изображения (двумерная сетка) и эту структуру данных можно рассматривать как граф. Ключевыми принципами сверточных нейронных сетей являются: локальная связь, разделяемые веса и использование многослойности. И эти принципы могут иметь большое значение при решении задач в области графов, поскольку графы являются наиболее типичной локально связной структурой, общие веса снижают вычислительные затраты по сравнению с традиционной теорией спектральных графов, а многослойная структура является ключом к работе с иерархическими структурами, которая отражает особенности различных размеров. 

Главной проблемой для обобщения сверточных нейронных сетей на графы является то, что графы, как правило, в общем случае представляют собой нерегулярную неевклидову структуру данных (число узлов и связей между ними произвольно). Эту проблему и призвана решить графовая нейронная сеть, поскольку CNN, не может должным образом обрабатывать ввод графа, т.к. сверточная нейронная сеть обрабатывает  узлы в определенном порядке. Однако, в графе нет естественного порядка, поэтому, чтобы полностью представить граф, мы должны пройти через все возможные порядки в качестве входных данных модели, которая будет очень избыточна при вычислениях. Для решения этой проблемы, GNN распространяются на каждом узле соответственно, игнорируя порядок ввода узлов. Другими словами, вывод GNN является инвариантным для порядка ввода узлов. 
\textbf{TODO: исправить предыдущий абзац}

Другим важным аспектом является использование графовых эмбеддингов, которые учатся представлять узлы, ребра или подграфы графа в низкоразмерных векторах. Однако, использование методов графовых эмбеддингов имеют два серьезных недостатка. Во-первых, никакие параметры не разделяются между узлами в кодировщике, что приводит к неэффективности вычислений, поскольку это означает, что число параметров растет линейно с количеством узлов. Во-вторых, методы векторного представления, такие как DeepWalk, Node2Vec \textbf{и т.д.}, не имеют возможности обобщения, что означает, что они не могут иметь дело с динамическими графами или обобщаться на новые графы.

Поэтому на основе сверточных нейронных сетях и графовых эмбеддингов были введены графовые нейронные сети коллективно агрегирующие информацию из структуры графов, при это делая это вычислительно более эффективно.

\subsection{4.1. Оригинальная концепция графовой нейронной сети}

В задаче классификации узлов графа, каждый узел $v$, как правило, характеризуется вектором-признаков $x_v$ и меткой класса $t_v$, к которому принадлежит узел. Имея частично размеченный граф $G$,
целью графовой нейронной сети является изучение скрытого представления каждого узла $v$, т.е. $h_v \in R^s$, зная информация о самом узле (признаки узла) и об окрестности узла. Полученное представление является $s$-мерным вектором и используется в дальнейшем для получения целевого значения $o_v$.  Более формально:
$$
h_v = f(x_v, x_{co[v]}, h_{ne[v]}, x_{ne[v]}) 
$$
где $x_v$- признаки узла $v$, $x_{co[v]}$ - признаки ребер, соединенных с узлом $v$, $h_{ne[v]}$ - векторное представление узлов, находящихся в окрестности узла $v$, и $x_{ne[v]}$ - признаки узлов, находящихся в окрестности узла $v$.  Функция $f$ является функцией перехода, которая проецирует входные данные в $s$-мерное пространство. 

Уравнение выше можно переписать как итеративный процесс, используя Банахову теорему о неподвижной точке \cite{fixed-point theorem}:
$$
H^{t+1} = F(H^t, X)
$$
$H$ и $X$ обозначают конкатенацию всех $h$ и $x$ соответственно.

Целевое значение графовой нейронной сети вычисляется путем передачи векторного представления $h_v$ и признаков $x_v$ в выходную функцию $g$:

$$
o_v = g(h_v, x_v)
$$
При этом, $f$ и $g$, могут быть интерпретированы как полносвязные нейронные сети прямого распространения. И параметры этих функций могут быть найдены как решение задачи минимизации функции потерь:
$$
	loss = \sum_{i=1}^p (t_i - o_i)
$$
которая может быть решена с помощью метода градиентного спуска.

Однако, оригинальный подход использования концепции графовой нейронной сети имеет существенные ограничения:
\begin{enumerate}
\item Она использует одни и те же параметры на итерации, в то время как большинство популярных нейронных сетей используют разные параметры в разных слоях, которые служат методом извлечения иерархических объектов. 
\item Она не может обрабатывать информацию о ребрах (например, разные типы ребра в графе могут указывать на разные отношения между узлами)
\item \textbf{TODO}
\end{enumerate}

Поэтому для решения вышеуказанных проблем было предложено более улучшенные вариации исходной концепции графовой нейронной сети. 


\subsection{4.2. GraphSAGE}

Большинство существующих подходов векторного представления узлов графа являются трансдуктивными, т.е. они подходят только для графов с фиксированной структурой. Однако при добавлении новых узлов в граф трансдуктивные методы приходится полностью переучивать, что является вычислительно затратным и неэффективным способом для больших графов.

Метод GraphSAGE \cite{GraphSAGE } успешно решает вышеупомянутую проблему. Этот подход базируется на сверточных нейронных сетях, обобщая их от обработки простых решеток до произвольных графов. 

Ключевое отличие от трансдуктивных подходов является то, что вместо того, чтобы обучать векторное представление для каждого узла, мы обучаем набор агрегирующих функций, которые учатся собирать информацию о признаках из окрестности узла. При этом, каждая агрегирующая функция собирает информацию из окрестности, которая находится на определенном расстоянии от целевого узла. 

Более формально, процесс получения векторного представления узлов представлен в Алгоритме \ref{algo:2}:

\begin{algorithm}[H]
		\KwData{Граф $G(V,E)$; признаки узлов $x_v, \forall v \in V$; весовая матрица $W^k, \forall k \in \{1, 2, ..., K\}$; агрегирующие функции $AGGREGATE_k, \forall k \in \{1, 2, ..., K\}$; нелинейная функция активации $\sigma$}
		\KwResult{Векторное представление $z_v, \forall v \in V$ }
  $ h_v^0 = x_v, \forall v \in V$ \\
  \For{k=1...K}{
      \ForEach{$v \in V$}{
          $h_{N(v)}^k = AGGREGATE_k({h_u^{k-1}, \forall u \in N(v)})$ \\
          $h_v^k = \sigma(W^k \cdot CONCAT(h_v^{k-1}, h_{N(v)}^k));$
      }
      $h_v^k = \frac{h_v^k}{||h_v^k||_2}, \forall v \in V$
  }
  $z_v = h_v^K, \forall v \in V$

    \caption{Алгоритм генерации эмбеддингов GraphSAGE}
    \label{algo:2}
\end{algorithm}

Каждый агрегатор во внешнем цикле отвечает за обработку узлов, находящихся на глубине $k$ от исходного узла и при этом обрабатываются векторные представления узлов, полученные на предыдущем шаге. После того, как агрегатор заканчивает свою работу мы получаем вектор $h_{N(v)}^k$, который конкатенируется с векторным представлением текущего обрабатываемого узла $h_v^{k-1}$, которое получено на предыдущем шаге. Полученный вектор проходит через полно-связный слой с нелинейной функцией активации $\sigma$, на выходе которой получается новое векторное представление $h_v^k$, которое используется на следующем шаге цикла.
Стоит заметить, что на первой итерации агрегирующая функция $AGGREGATE_1$ в качестве эмбеддингов использует признаки узлов $x_v$.  

Важным моментом является выбор агрегирующей функции и авторы исходной статье говорят, что такая функция должна обладать свойством симметрии, поскольку это гарантирует, что наша модель нейронной сети может быть обучена и применена к произвольно упорядоченному набору узлов. В качестве агрегирующих функций может выбрана одна из следующих трех функций:
\begin{enumerate}
\item Mean агрегатор. Суть данного агрегатора заключается в том, что берется просто поэлементное среднее векторов.
\item LSTM агрератор. Данный агрегатор использует архитектуру, которая используется в нейронных сетях с долгой краткосрочной памятью, но поскольку данная архитектура не является симметричной, поскольку она обрабатывает фиксированную последовательность в определенном порядке, то при использовании данного метода применяется случайная перестановка.
\item Pooling агрегатор.  При таком подходе векторное представление каждого узла проходит через полно-связный слой нейронной сети и после этого преобразования применяется поэлементная операция max-пулинга.
\end{enumerate}



\clearpage